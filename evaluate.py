#!/usr/bin/env python3
"""
è¯„æµ‹è„šæœ¬ - ç”¨äºè¯„ä¼°è§†é¢‘AIé—®ç­”ç³»ç»Ÿçš„æ€§èƒ½
"""

import json
import time
import requests
import re
from typing import Dict, List, Any
import os
import sys


# APIé…ç½®
API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8000")
AGENT_CHAT_URL = f"{API_BASE_URL}/api/v1/agent/chat"


def load_test_cases() -> List[Dict[str, Any]]:
    """åŠ è½½æµ‹è¯•ç”¨ä¾‹æ•°æ®"""
    try:
        with open('data/test_cases.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('test_cases', [])
    except FileNotFoundError:
        print("é”™è¯¯: æ‰¾ä¸åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶ data/test_cases.json")
        return []
    except json.JSONDecodeError:
        print("é”™è¯¯: æµ‹è¯•æ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯")
        return []


def check_keywords(answer: str, expected_keywords: List[str]) -> float:
    """
    æ£€æŸ¥ç­”æ¡ˆä¸­å…³é”®è¯çš„è¦†ç›–ç‡

    Args:
        answer: ç³»ç»Ÿç”Ÿæˆçš„ç­”æ¡ˆ
        expected_keywords: æœŸæœ›åŒ…å«çš„å…³é”®è¯åˆ—è¡¨

    Returns:
        å…³é”®è¯è¦†ç›–ç‡ (0.0 - 1.0)
    """
    if not expected_keywords:
        return 1.0

    found_count = 0
    for keyword in expected_keywords:
        if keyword.lower() in answer.lower():
            found_count += 1

    return found_count / len(expected_keywords)


def extract_citations(answer: str) -> int:
    """
    ä»ç­”æ¡ˆä¸­æå–å¼•ç”¨æ•°é‡

    æŸ¥æ‰¾ç­”æ¡ˆä¸­çš„å¼•ç”¨æ ‡è®°ï¼Œå¦‚ [1], (æ¥æº:xxx) ç­‰
    """
    # åŒ¹é…å„ç§å¼•ç”¨æ ¼å¼
    patterns = [
        r'\[\d+\]',  # [1], [2] ç­‰
        r'\(æ¥æº:.*?\)',  # (æ¥æº:xxx)
        r'å¼•ç”¨è‡ª.*?[\n\.]',  # å¼•ç”¨è‡ªxxx
        r'å‚è€ƒ.*?[\n\.]',  # å‚è€ƒxxx
    ]

    citations = 0
    for pattern in patterns:
        citations += len(re.findall(pattern, answer))

    return citations


def call_agent_api(question: str, use_mock: bool = False) -> Dict[str, Any]:
    """
    è°ƒç”¨Agent APIè¿›è¡Œé—®ç­”

    Args:
        question: ç”¨æˆ·é—®é¢˜
        use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰

    Returns:
        åŒ…å«ç­”æ¡ˆå’Œå…ƒæ•°æ®çš„å­—å…¸
    """
    if use_mock:
        # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        import random
        latency = random.uniform(500, 3000)
        time.sleep(latency / 1000)

        mock_answers = {
            "ç³»ç»Ÿæ”¯æŒå“ªäº›è§†é¢‘æ–‡ä»¶æ ¼å¼?": "ç³»ç»Ÿæ”¯æŒMP4ã€MOVã€AVIã€WebMå’ŒMKVç­‰å¸¸è§è§†é¢‘æ ¼å¼ã€‚",
            "å¦‚ä½•ä¸Šä¼ è§†é¢‘æ–‡ä»¶è¿›è¡Œå¤„ç†?": "æ‚¨å¯ä»¥é€šè¿‡ä¸Šä¼ æ¥å£ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å°†å…¶åŠ å…¥å¤„ç†é˜Ÿåˆ—è¿›è¡Œè½¬ç ã€‚",
            "è¯­éŸ³è¯†åˆ«æ”¯æŒå“ªäº›è¯­è¨€?": "ç³»ç»Ÿæ”¯æŒä¸­æ–‡ç­‰å¤šç§è¯­è¨€ï¼Œä½¿ç”¨é˜¿é‡ŒFunASRå¼•æ“è¿›è¡Œé«˜ç²¾åº¦è¯­éŸ³è¯†åˆ«ã€‚",
        }

        answer = mock_answers.get(question, f"å…³äº'{question}'ï¼Œç³»ç»Ÿä½¿ç”¨RAGæŠ€æœ¯å’Œå‘é‡æ£€ç´¢è¿›è¡Œæ™ºèƒ½é—®ç­”ã€‚")
        tokens = random.randint(100, 800)

        return {
            "answer": answer,
            "metadata": {
                "latency_ms": latency,
                "total_tokens": tokens,
                "has_citations": random.choice([True, False])
            }
        }

    # çœŸå®APIè°ƒç”¨
    start_time = time.time()

    try:
        response = requests.post(
            AGENT_CHAT_URL,
            json={"message": question},
            timeout=60
        )

        latency_ms = (time.time() - start_time) * 1000

        if response.status_code == 200:
            data = response.json()
            answer = data.get("response", "")

            # å°è¯•ä»metadataä¸­è·å–tokenä¿¡æ¯
            metadata = data.get("metadata", {})
            tokens = metadata.get("response_length", len(answer.split()))

            return {
                "answer": answer,
                "metadata": {
                    "latency_ms": latency_ms,
                    "total_tokens": tokens,
                    "processing_time": data.get("processing_time", 0),
                    "has_citations": extract_citations(answer) > 0
                }
            }
        else:
            print(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return {
                "answer": "",
                "metadata": {
                    "latency_ms": latency_ms,
                    "total_tokens": 0,
                    "error": response.text
                }
            }
    except Exception as e:
        latency_ms = (time.time() - start_time) * 1000
        print(f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
        return {
            "answer": "",
            "metadata": {
                "latency_ms": latency_ms,
                "total_tokens": 0,
                "error": str(e)
            }
        }


def calculate_cost(avg_tokens: float, model_type: str = "doubao") -> float:
    """
    è®¡ç®—æ¯ä¸ªæŸ¥è¯¢çš„å¹³å‡æˆæœ¬

    Args:
        avg_tokens: å¹³å‡tokenæ•°é‡
        model_type: æ¨¡å‹ç±»å‹ (doubao/gpt4)
    """
    # ä¸åŒæ¨¡å‹çš„å®šä»·ï¼ˆæ¯1000 tokensçš„ä»·æ ¼ï¼Œå•ä½ï¼šç¾å…ƒï¼‰
    pricing = {
        "doubao": 0.008,  # ç«å±±å¼•æ“è±†åŒ…æ¨¡å‹
        "gpt4": 0.03,     # GPT-4
        "gpt35": 0.002    # GPT-3.5
    }

    cost_per_1k_tokens = pricing.get(model_type, 0.01)
    return (avg_tokens / 1000) * cost_per_1k_tokens


def evaluate_system(
    use_mock: bool = False,
    max_cases: int = None,
    difficulty_filter: str = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œç³»ç»Ÿè¯„æµ‹

    Args:
        use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        max_cases: æœ€å¤§æµ‹è¯•ç”¨ä¾‹æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        difficulty_filter: éš¾åº¦è¿‡æ»¤å™¨ ("easy", "medium", "hard")

    Returns:
        åŒ…å«è¯„æµ‹æŒ‡æ ‡çš„å­—å…¸
    """
    test_cases = load_test_cases()
    if not test_cases:
        return {"error": "æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç”¨ä¾‹"}

    # åº”ç”¨éš¾åº¦è¿‡æ»¤
    if difficulty_filter:
        test_cases = [c for c in test_cases if c.get("difficulty") == difficulty_filter]
        print(f"è¿‡æ»¤åˆ° {len(test_cases)} ä¸ª '{difficulty_filter}' éš¾åº¦çš„æµ‹è¯•ç”¨ä¾‹")

    # é™åˆ¶æµ‹è¯•ç”¨ä¾‹æ•°é‡
    if max_cases:
        test_cases = test_cases[:max_cases]

    results = []
    total_cases = len(test_cases)

    print(f"\nå¼€å§‹è¯„æµ‹ï¼Œå…± {total_cases} ä¸ªæµ‹è¯•ç”¨ä¾‹...")
    print(f"APIåœ°å€: {AGENT_CHAT_URL}")
    print(f"æ¨¡å¼: {'æ¨¡æ‹Ÿæ•°æ®' if use_mock else 'çœŸå®APIè°ƒç”¨'}\n")

    # ç”¨äºç»Ÿè®¡
    successful_cases = 0
    failed_cases = 0
    citations_found = 0

    for i, case in enumerate(test_cases, 1):
        question = case["question"]
        print(f"[{i}/{total_cases}] é—®é¢˜: {question[:60]}...")

        try:
            # è°ƒç”¨é—®ç­”ç³»ç»Ÿ
            response = call_agent_api(question, use_mock=use_mock)

            # æ£€æŸ¥æ˜¯å¦æœ‰ç­”æ¡ˆ
            if not response["answer"]:
                print(f"  âš ï¸  æœªè·å–åˆ°ç­”æ¡ˆ")
                failed_cases += 1
                continue

            # æ£€æŸ¥å…³é”®è¯è¦†ç›–ç‡
            accuracy = check_keywords(response["answer"], case["expected_keywords"])

            # è·å–å»¶è¿Ÿå’Œtokenä¿¡æ¯
            latency = response["metadata"]["latency_ms"]
            tokens = response["metadata"]["total_tokens"]
            has_citations = response["metadata"].get("has_citations", False)

            if has_citations:
                citations_found += 1

            results.append({
                "id": case["id"],
                "question": question,
                "answer": response["answer"][:100] + "...",  # åªä¿å­˜å‰100å­—ç¬¦
                "accuracy": accuracy,
                "latency": latency,
                "tokens": tokens,
                "difficulty": case.get("difficulty", "unknown"),
                "has_citations": has_citations
            })

            successful_cases += 1
            print(f"  âœ“ å‡†ç¡®ç‡: {accuracy:.2%}, å»¶è¿Ÿ: {latency:.0f}ms, Tokens: {tokens}")

        except Exception as e:
            print(f"  âœ— å¤„ç†å¤±è´¥: {str(e)}")
            failed_cases += 1
            continue

    if not results:
        return {"error": "æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹éƒ½å¤±è´¥äº†"}

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_accuracy = sum(r["accuracy"] for r in results) / len(results)
    avg_latency = sum(r["latency"] for r in results) / len(results)
    avg_tokens = sum(r["tokens"] for r in results) / len(results)
    cost_per_query = calculate_cost(avg_tokens)

    # è®¡ç®—å¼•ç”¨å‡†ç¡®ç‡ï¼ˆæœ‰å¼•ç”¨çš„æ¯”ä¾‹ï¼‰
    citation_accuracy = citations_found / len(results) if results else 0

    # æŒ‰éš¾åº¦åˆ†ç»„ç»Ÿè®¡
    difficulty_stats = {}
    for difficulty in ["easy", "medium", "hard"]:
        difficulty_results = [r for r in results if r.get("difficulty") == difficulty]
        if difficulty_results:
            difficulty_stats[difficulty] = {
                "count": len(difficulty_results),
                "avg_accuracy": sum(r["accuracy"] for r in difficulty_results) / len(difficulty_results),
                "avg_latency_ms": sum(r["latency"] for r in difficulty_results) / len(difficulty_results)
            }

    # æ„å»ºè¯„æµ‹ç»“æœ
    evaluation_result = {
        "accuracy": round(avg_accuracy, 2),
        "avg_latency_ms": round(avg_latency),
        "avg_tokens": round(avg_tokens),
        "cost_per_query": round(cost_per_query, 4),
        "citation_accuracy": round(citation_accuracy, 2),
        "total_cases": total_cases,
        "successful_cases": successful_cases,
        "failed_cases": failed_cases,
        "difficulty_breakdown": difficulty_stats
    }

    return evaluation_result


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è§†é¢‘AIé—®ç­”ç³»ç»Ÿè¯„æµ‹å·¥å…·')
    parser.add_argument('--mock', action='store_true', help='ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•')
    parser.add_argument('--max-cases', type=int, help='æœ€å¤§æµ‹è¯•ç”¨ä¾‹æ•°é‡')
    parser.add_argument('--difficulty', choices=['easy', 'medium', 'hard'], help='åªæµ‹è¯•æŒ‡å®šéš¾åº¦')
    parser.add_argument('--output', default='evaluation_result.json', help='ç»“æœè¾“å‡ºæ–‡ä»¶')

    args = parser.parse_args()

    print("=" * 60)
    print("       è§†é¢‘AIé—®ç­”ç³»ç»Ÿè¯„æµ‹å·¥å…· - QuickRewind")
    print("=" * 60)

    # æ‰§è¡Œè¯„æµ‹
    result = evaluate_system(
        use_mock=args.mock,
        max_cases=args.max_cases,
        difficulty_filter=args.difficulty
    )

    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 60)
    print("                    è¯„æµ‹ç»“æœ")
    print("=" * 60)

    if "error" in result:
        print(f"âŒ è¯„æµ‹å¤±è´¥: {result['error']}")
        sys.exit(1)

    # æ ¼å¼åŒ–è¾“å‡ºå…³é”®æŒ‡æ ‡
    print(f"\nğŸ“Š æ ¸å¿ƒæŒ‡æ ‡:")
    print(f"  å‡†ç¡®ç‡ (Accuracy):           {result['accuracy']:.2%}")
    print(f"  å¹³å‡å»¶è¿Ÿ (Avg Latency):      {result['avg_latency_ms']:.0f} ms")
    print(f"  å¹³å‡Token (Avg Tokens):      {result['avg_tokens']:.0f}")
    print(f"  å•æ¬¡æˆæœ¬ (Cost per Query):   ${result['cost_per_query']:.4f}")
    print(f"  å¼•ç”¨å‡†ç¡®ç‡ (Citation Acc):   {result['citation_accuracy']:.2%}")

    print(f"\nğŸ“ˆ æµ‹è¯•ç»Ÿè®¡:")
    print(f"  æ€»ç”¨ä¾‹æ•°:   {result['total_cases']}")
    print(f"  æˆåŠŸæ•°:     {result['successful_cases']}")
    print(f"  å¤±è´¥æ•°:     {result['failed_cases']}")

    if result.get('difficulty_breakdown'):
        print(f"\nğŸ“Š éš¾åº¦åˆ†æ:")
        for difficulty, stats in result['difficulty_breakdown'].items():
            print(f"  {difficulty.capitalize():8s}: å‡†ç¡®ç‡ {stats['avg_accuracy']:.2%}, "
                  f"å»¶è¿Ÿ {stats['avg_latency_ms']:.0f}ms ({stats['count']}ä¸ªç”¨ä¾‹)")

    # ä¿å­˜å®Œæ•´ç»“æœåˆ°æ–‡ä»¶
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å®Œæ•´è¯„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
    print("=" * 60)


if __name__ == "__main__":
    main()